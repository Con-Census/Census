{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38a4b7f3",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "- The South region has most diversed cost versus pop_density\n",
    "- The South region has, as the author Cindy Ermus wrote in her book,\"The Gulf South, and the Gulf Coast in particular, is bound together by much more than geography or the shared experience of risk and vulnerability to wind, water, erosion, and biological exchanges,” she writes. “More fundamentally, the environment has helped define the region’s identity and largely determined its history, its social fabric, and its economy.” We can assumed, based on this graph that south region has significant high risk compare to other regions.\n",
    "- The New York and California have the highest support_level. A seperated studies are recommended\n",
    "- Climate change should be our national top concern, and higher amount of funding should be allocated.\n",
    "- The KNN is our best model and we use it in out final test. The KNN model achived 90% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f8ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os and csv functions\n",
    "import os\n",
    "import csv\n",
    "# import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tabula import read_pdf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "# utilized for creating models and visualization\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# utilized for metrics on my models\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Homemade module\n",
    "import prepare\n",
    "# explore.py\n",
    "import modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300c053",
   "metadata": {},
   "source": [
    "# Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a097a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the file from local\n",
    "df=prepare.get_fema_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2904a12",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'stateabbrv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# wrangle df\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mprepare\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_fema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codeup-data-science/Census/prepare.py:217\u001b[0m, in \u001b[0;36mprepare_fema\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m    214\u001b[0m efunds \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mDataFrame(efunds)\n\u001b[1;32m    216\u001b[0m \u001b[38;5;66;03m# Merge\u001b[39;00m\n\u001b[0;32m--> 217\u001b[0m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mefunds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstateabbrv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mright\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m df\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:9345\u001b[0m, in \u001b[0;36mDataFrame.merge\u001b[0;34m(self, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m   9326\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   9327\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m)\n\u001b[1;32m   9328\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   9341\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   9342\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[1;32m   9343\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmerge\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m merge\n\u001b[0;32m-> 9345\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mmerge\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   9346\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9347\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9348\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9349\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9350\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9351\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9352\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9353\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9354\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9355\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9356\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9357\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9358\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   9359\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:107\u001b[0m, in \u001b[0;36mmerge\u001b[0;34m(left, right, how, on, left_on, right_on, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mleft : DataFrame or named Series\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_merge_doc, indents\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mmerge\u001b[39m(\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    105\u001b[0m     validate: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    106\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m--> 107\u001b[0m     op \u001b[38;5;241m=\u001b[39m \u001b[43m_MergeOperation\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    108\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    110\u001b[0m \u001b[43m        \u001b[49m\u001b[43mhow\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mhow\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    111\u001b[0m \u001b[43m        \u001b[49m\u001b[43mon\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    112\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    113\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_on\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_on\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    114\u001b[0m \u001b[43m        \u001b[49m\u001b[43mleft_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mleft_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    115\u001b[0m \u001b[43m        \u001b[49m\u001b[43mright_index\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mright_index\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    116\u001b[0m \u001b[43m        \u001b[49m\u001b[43msort\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msort\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    117\u001b[0m \u001b[43m        \u001b[49m\u001b[43msuffixes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msuffixes\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    118\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcopy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    119\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindicator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindicator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    120\u001b[0m \u001b[43m        \u001b[49m\u001b[43mvalidate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mvalidate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    121\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m op\u001b[38;5;241m.\u001b[39mget_result()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:700\u001b[0m, in \u001b[0;36m_MergeOperation.__init__\u001b[0;34m(self, left, right, how, on, left_on, right_on, axis, left_index, right_index, sort, suffixes, copy, indicator, validate)\u001b[0m\n\u001b[1;32m    693\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cross \u001b[38;5;241m=\u001b[39m cross_col\n\u001b[1;32m    695\u001b[0m \u001b[38;5;66;03m# note this function has side effects\u001b[39;00m\n\u001b[1;32m    696\u001b[0m (\n\u001b[1;32m    697\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mleft_join_keys,\n\u001b[1;32m    698\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mright_join_keys,\n\u001b[1;32m    699\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mjoin_names,\n\u001b[0;32m--> 700\u001b[0m ) \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_merge_keys\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    702\u001b[0m \u001b[38;5;66;03m# validate the merge keys dtypes. We may need to coerce\u001b[39;00m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;66;03m# to avoid incompatible dtypes\u001b[39;00m\n\u001b[1;32m    704\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_maybe_coerce_merge_keys()\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/reshape/merge.py:1110\u001b[0m, in \u001b[0;36m_MergeOperation._get_merge_keys\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1108\u001b[0m     right_keys\u001b[38;5;241m.\u001b[39mappend(rk)\n\u001b[1;32m   1109\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lk \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1110\u001b[0m     left_keys\u001b[38;5;241m.\u001b[39mappend(\u001b[43mleft\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_label_or_level_values\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlk\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m   1111\u001b[0m     join_names\u001b[38;5;241m.\u001b[39mappend(lk)\n\u001b[1;32m   1112\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1113\u001b[0m     \u001b[38;5;66;03m# work-around for merge_asof(left_index=True)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:1840\u001b[0m, in \u001b[0;36mNDFrame._get_label_or_level_values\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1838\u001b[0m     values \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maxes[axis]\u001b[38;5;241m.\u001b[39mget_level_values(key)\u001b[38;5;241m.\u001b[39m_values\n\u001b[1;32m   1839\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1840\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key)\n\u001b[1;32m   1842\u001b[0m \u001b[38;5;66;03m# Check for duplicates\u001b[39;00m\n\u001b[1;32m   1843\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m values\u001b[38;5;241m.\u001b[39mndim \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "\u001b[0;31mKeyError\u001b[0m: 'stateabbrv'"
     ]
    }
   ],
   "source": [
    "# wrangle df\n",
    "df=prepare.prepare_fema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90538ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution\n",
    "df.hist(figsize=(24, 10), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76962c5c",
   "metadata": {},
   "source": [
    "- The majority of people has less than $8,000 per person\n",
    "- The distribution graphs showing a lot outliers in this data. We decided not remove any outliers due to we would like all inclusive data for all states and counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df into test (20%) and train_validate (80%)\n",
    "tv_df, test = train_test_split(df, test_size=0.2, random_state=123)\n",
    "# split train_validate off into train (70% of 80% = 56%) and validate (30% of 80% = 24%)\n",
    "train, validate = train_test_split(tv_df, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71546681",
   "metadata": {},
   "source": [
    "# Wrangle Key Takeaway\n",
    "- The data has 3142 rows, and 26 columns. This project specifically focusing on the Severe Storm, Draught and Hurricane.\n",
    "- The project has included all 50 states' counties and the District of Columbia area. The risk score is retrived it from Federal Emergency Management Agency (FEMA). In our analysis we used three disasters: Drought, Severe Storm, and Hurricane.\n",
    "- Next, we will explore the data and set our target variable==support level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539c612",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38623724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a map before out exploration\n",
    "fig = px.choropleth(df,\n",
    "                    locations='state', \n",
    "                    locationmode=\"USA-states\", \n",
    "                    scope=\"usa\",\n",
    "                    color='support_value',\n",
    "                    color_continuous_scale=\"rdylbu\", \n",
    "                    \n",
    "                    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a5911",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "- The science.org research believes that the california will face the mega-flood in near future as climate change worsen\n",
    "    - 'Recent evidence suggests that increases in western United States flood risk caused by anthropogenic warming may have been counteracted in recent decades by natural variability, but that further warming and shifts in natural variability will eventually “unmask” this accumulated increase in regional flood risk (51). Additional work suggests that the response of flood risk to climate change is likely to exhibit threshold behavior, at least in certain climatological and hydrological regimes (52), with a precipitation extremeness threshold dictating whether flood risk decreases (for smaller events, due to the antecedent soil aridification effect of warming temperatures) or increases (for the largest events, due to the overwhelming effect of large increases in precipitation intensity). Both of these considerations are especially germane to California' [science.org](https://www.science.org/doi/10.1126/sciadv.abq0995)\n",
    "- The columbia study and New York Times journalist argued that the New York is not adequately prepared for the next storm like Hurricane Sandy.\n",
    "    - 'The barrier debate comes as New York City is still struggling to respond to Sandy, and the larger need to carefully reshape an entire region’s infrastructure to adapt to climate change. In the more than seven years since the storm killed 72 people and caused 62 billion in damage, agencies have spent just 54 percent of the 14.7 billion allocated by the federal government to help the city recover and prepare for new storms.' [columbia.edu](https://news.climate.columbia.edu/2020/01/21/politics-cost-adapting-climate-change-new-york-city/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0cfb6",
   "metadata": {},
   "source": [
    "### Q1: Whether the dense of the population contributed to the overall funding available or the risk level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee43489d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up th eenvironment\n",
    "q1=tv_df.groupby('state',as_index=False).agg(sum)\n",
    "q1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin the risk level to identify whether the funding available also associate with \n",
    "q1['risk_bin'] = pd.qcut(q1.risk_score, 4, labels=['low', 'median', 'hight', 'extremely_high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use catplot\n",
    "sns.pointplot(data=q1, x=\"risk_bin\", y=\"pop_density\", dodge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1f40c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use catplot\n",
    "sns.pointplot(data=q1, x=\"risk_bin\", y=\"state_funding\", dodge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e051045",
   "metadata": {},
   "source": [
    "### q1 Key Takeaway\n",
    "- As the risk level rise, the funding will be impacted by the pop_density\n",
    "- The extremely high risk has highest funding when the pop_density is high\n",
    "- It is worth to further explore which region it is belong to and what kind of the disaster contributed to increased funding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8106cd2",
   "metadata": {},
   "source": [
    "### q2 the cost of the disaster per region per risk level per pop_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the environment\n",
    "q2 = tv_df.copy()\n",
    "q2['risk_bin'] = pd.qcut(q2.risk_score, 4, labels=['low', 'median', 'hight', 'extremely_high'])\n",
    "\n",
    "# create the region based on the state's location\n",
    "west = ['WA','OR','CA','ID','NV','MT','WY','UT','AZ','CO','NM']\n",
    "midwest = ['ND','MN','WI','MI','SD','NE','KS','IA','MO','IL','IN','OH']\n",
    "south = ['TX','OK','AR','LA','MS','TN','KY','AL','GA','FL','SC','NC','VA','WV','MD','DE','DC']\n",
    "northeast = ['PA','NJ','NY','CT','MA','RI','VT','NH','ME']\n",
    "west_dict = {i:\"west\" for i in west}\n",
    "midwest_dict = {i:\"midwest\" for i in midwest}\n",
    "south_dict = {i:\"south\" for i in south}\n",
    "northeast_dict = {i:\"northeast\" for i in northeast}\n",
    "d = {**west_dict, **midwest_dict, **south_dict, **northeast_dict}\n",
    "q2['Region'] = q2['state'].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23438174",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(data=q2, x=\"pop_density\", y=\"cost\", hue='Region')\n",
    "g.plot_joint(sns.kdeplot, color=\"r\", zorder=0, levels=6)\n",
    "g.plot_marginals(sns.rugplot, color=\"r\", height=-.15, clip_on=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7dfdd",
   "metadata": {},
   "source": [
    "### q2 Key Takeaway\n",
    "- The South region has most diversed cost versus pop_density\n",
    "- The South region has as the author [Cindy Ermus wrote](https://www.theatlantic.com/technology/archive/2017/08/why-the-gulf-coast-is-uniquely-vulnerable-to-disasters-hurricane-harvey/538374/) in her book,\"The Gulf South, and the Gulf Coast in particular, is bound together by much more than geography or the shared experience of risk and vulnerability to wind, water, erosion, and biological exchanges,” she writes. “More fundamentally, the environment has helped define the region’s identity and largely determined its history, its social fabric, and its economy.”\n",
    "- We can assumed, based on this graph that south region has significant high support value compare to other regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6e733",
   "metadata": {},
   "source": [
    "### q3: Based on the q2 takeaway, we are going to make a hypothesis test that whether the south region has higher support value compare to the other regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a561b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the environment\n",
    "q3 = tv_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df00fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the region based on the state's location\n",
    "west = ['WA','OR','CA','ID','NV','MT','WY','UT','AZ','CO','NM']\n",
    "midwest = ['ND','MN','WI','MI','SD','NE','KS','IA','MO','IL','IN','OH']\n",
    "south = ['TX','OK','AR','LA','MS','TN','KY','AL','GA','FL','SC','NC','VA','WV','MD','DE','DC']\n",
    "northeast = ['PA','NJ','NY','CT','MA','RI','VT','NH','ME']\n",
    "west_dict = {i:\"west\" for i in west}\n",
    "midwest_dict = {i:\"midwest\" for i in midwest}\n",
    "south_dict = {i:\"south\" for i in south}\n",
    "northeast_dict = {i:\"northeast\" for i in northeast}\n",
    "d = {**west_dict, **midwest_dict, **south_dict, **northeast_dict}\n",
    "q3['Region'] = q3['state'].map(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f3d72",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "- H0: South Region's risk score equals or less than non-south region risk score\n",
    "- Ha: South Region's risk score greater than non-south region risk score\n",
    "- Significance Level α is already set to .05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf24750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "south_r = q3[q3.Region == 'south'].support_value\n",
    "non_south= q3[q3.Region != 'south'].support_value\n",
    "west=q3[q3.Region == 'west'].support_value\n",
    "midwest=q3[q3.Region == 'midwest'].support_value\n",
    "northeast=q3[q3.Region == 'northeast'].support_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6453a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check variance\n",
    "print(south_r.var())\n",
    "print(non_south.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81360b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into the test\n",
    "t, p = stats.ttest_ind(south_r, non_south, equal_var=False)\n",
    "t, p\n",
    "p<0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c8338",
   "metadata": {},
   "source": [
    "### We reject the null value, that the sourth region's risk score is greater than other regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dfa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kruskal(south, west,northeast,midwest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6dab08",
   "metadata": {},
   "source": [
    "### Using Kruskal-Wallis test, non-parametric test for ANOVA, also shows us that the mean of Risk Score  from the 4 Regions is significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb401ae",
   "metadata": {},
   "source": [
    "### Q3 takeaway\n",
    "- The south region risk score is higher than other regions\n",
    "- The Kruskal-Wallis test also concur that the risk score is significantly different among 4 regions\n",
    "- We can further assumed the supporting level in south region is higher than other region "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa043665",
   "metadata": {},
   "source": [
    "### q4: Based on the q3 takeaway, we are going to make a hypothesis test that whether the south region has higher supporting level compare to the other regions based on their risk_level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "q4 = tv_df.copy()\n",
    "q4['risk_level'] = pd.qcut(q4.risk_score, 4, labels=['low', 'median', 'hight', 'extremely_high'])\n",
    "# create the region based on the state's location\n",
    "west = ['WA','OR','CA','ID','NV','MT','WY','UT','AZ','CO','NM']\n",
    "midwest = ['ND','MN','WI','MI','SD','NE','KS','IA','MO','IL','IN','OH']\n",
    "south = ['TX','OK','AR','LA','MS','TN','KY','AL','GA','FL','SC','NC','VA','WV','MD','DE','DC']\n",
    "northeast = ['PA','NJ','NY','CT','MA','RI','VT','NH','ME']\n",
    "west_dict = {i:\"west\" for i in west}\n",
    "midwest_dict = {i:\"midwest\" for i in midwest}\n",
    "south_dict = {i:\"south\" for i in south}\n",
    "northeast_dict = {i:\"northeast\" for i in northeast}\n",
    "d = {**west_dict, **midwest_dict, **south_dict, **northeast_dict}\n",
    "q4['Region'] = q4['state'].map(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48721297",
   "metadata": {},
   "source": [
    "H0: support level is not depended on the risk level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create observed dataframe\n",
    "observed = pd.crosstab(q4.risk_level, q4.Region)\n",
    "observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi2 test\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "print('Observed\\n')\n",
    "print(observed.values)\n",
    "print('---\\nExpected\\n')\n",
    "print(expected)\n",
    "print('---\\n')\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p}')\n",
    "print('P is less than Significance Level:',p<0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b467822",
   "metadata": {},
   "source": [
    "### Q4 key takeawat\n",
    "- The support level is depended on the risk score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5780485",
   "metadata": {},
   "source": [
    "# Exploration Key takeaway\n",
    "- The south region has higher support level in average compare to the other regions\n",
    "- The risk level is associated with support level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebd369",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data and set up the environment for our scaling\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = modeling.train_validate_test(df, 'support_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02beb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data for the modeling\n",
    "X_train_scaled, X_validate_scaled, X_test_scaled = modeling.scale_data(X_train, X_validate, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3245a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the baseline for the model\n",
    "(y_train == 'bottom tier').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a450065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up my baseline accuracy.  Models must have a better accuracy than this established baseline.\n",
    "print(f'The baseline accuracy for bottom tier counties in all cases within the Climate Risk Assessment dataset is {(y_train == \"bottom tier\").mean():.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0880c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use created module to train with different models and find out which model has best accuracy.\n",
    "modeling.modeling(X_train_scaled, y_train, X_validate_scaled, y_validate, 5, 3, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d2e6c",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "- The Kneighbors Classifier has the highest accuracy when we have it with max_depth=3 and random_state=123\n",
    "- We will going to use the KNN model for our final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed96374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting KNN into final test\n",
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "pred_test = knn.predict(X_test_scaled)\n",
    "print('KNN')\n",
    "print('')\n",
    "print('test score: ')\n",
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "actual_test=y_test.copy()\n",
    "ConfusionMatrixDisplay(confusion_matrix(actual_test,pred_test),display_labels=knn.classes_).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the dataset with our random forest model\n",
    "prob_test = knn.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ac6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a probability dataframe\n",
    "prob_df = pd.DataFrame(prob_test, columns=knn.classes_.tolist())\n",
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the indext\n",
    "reset_test =test.reset_index()\n",
    "reset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7771063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put two dataframe together\n",
    "test_prob_df = pd.concat([reset_test, prob_df], axis=1)\n",
    "test_prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a426227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the propability result to create a column call predicted\n",
    "test_prob_df['predicted'] = pred_test\n",
    "test_prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive columns of interest\n",
    "# create the region based on the state's location\n",
    "west = ['WA','OR','CA','ID','NV','MT','WY','UT','AZ','CO','NM']\n",
    "midwest = ['ND','MN','WI','MI','SD','NE','KS','IA','MO','IL','IN','OH']\n",
    "south = ['TX','OK','AR','LA','MS','TN','KY','AL','GA','FL','SC','NC','VA','WV','MD','DE','DC']\n",
    "northeast = ['PA','NJ','NY','CT','MA','RI','VT','NH','ME']\n",
    "west_dict = {i:\"west\" for i in west}\n",
    "midwest_dict = {i:\"midwest\" for i in midwest}\n",
    "south_dict = {i:\"south\" for i in south}\n",
    "northeast_dict = {i:\"northeast\" for i in northeast}\n",
    "d = {**west_dict, **midwest_dict, **south_dict, **northeast_dict}\n",
    "test_prob_df['Region'] = test_prob_df['state'].map(d)\n",
    "csv_df = test_prob_df[['support_level', 'Region','predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cdfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataframe\n",
    "csv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d6676",
   "metadata": {},
   "source": [
    "### Modeling Takeaway\n",
    "- The KNN is our best model and we use it in out final test\n",
    "- The KNN model achived 90% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4654f189",
   "metadata": {},
   "source": [
    "# Recommendation, Conclusion, and Next Step\n",
    "\n",
    "## Recommendation\n",
    "- New York and California have the highest support_level. A separate study is recommended for these specific states and their counties within.\n",
    "- Climate change should be our top concern at the all levels of government, and higher amount of funding should be allocated accordingly.\n",
    "- The KNN is our best model in our training, so we used it in our test. The model achived 90% accuracy \n",
    "\n",
    "## Conclusion \n",
    "- The initial data indicated that the New York and California have the highest risk in the United States in terms of funding and studies were found to back up those findings.\n",
    "- The South region has the most diverse cost in damages versus pop_density\n",
    "- The South region has, as the author Cindy Ermus wrote in her book,\"The Gulf South, and the Gulf Coast in particular, is bound together by much more than geography or the shared experience of risk and vulnerability to wind, water, erosion, and biological exchanges,” she writes. “More fundamentally, the environment has helped define the region’s identity and largely determined its history, its social fabric, and its economy.” We can assumed, based on this graph that south region has significant high risk compare to other regions\n",
    "\n",
    "## Next Step\n",
    "- Expand the scope of the data (include more hazards and possibly more geospatial data)\n",
    "- Improve the machine learning model by using other algorithms\n",
    "- Conduct a seperate study regarding New York and California or possibly introduce a model that can look at specific regions at any given time according to the most common hazards found in the area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4540e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
