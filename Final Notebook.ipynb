{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38a4b7f3",
   "metadata": {},
   "source": [
    "# Executive Summary\n",
    "- The South region has most diversed cost versus pop_density\n",
    "- The South region has, as the author Cindy Ermus wrote in her book,\"The Gulf South, and the Gulf Coast in particular, is bound together by much more than geography or the shared experience of risk and vulnerability to wind, water, erosion, and biological exchanges,” she writes. “More fundamentally, the environment has helped define the region’s identity and largely determined its history, its social fabric, and its economy.” We can assumed, based on this graph that south region has significant high risk compare to other regions.\n",
    "- The New York and California have the highest support_level. A seperated studies are recommended\n",
    "- Climate change should be our national top concern, and higher amount of funding should be allocated.\n",
    "- The KNN is our best model and we use it in out final test. The KNN model achived 90% accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "97f8ab66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os and csv functions\n",
    "import os\n",
    "import csv\n",
    "# import libaries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tabulate import tabulate\n",
    "from datetime import datetime\n",
    "from sklearn.impute import SimpleImputer\n",
    "from tabula import read_pdf\n",
    "from sklearn.model_selection import train_test_split\n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.express as px\n",
    "# utilized for creating models and visualization\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "# utilized for metrics on my models\n",
    "from sklearn.metrics import confusion_matrix, classification_report, ConfusionMatrixDisplay\n",
    "# ignore warnings\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# Homemade module\n",
    "import prepare\n",
    "# explore.py\n",
    "import modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1300c053",
   "metadata": {},
   "source": [
    "# Wrangle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "12a097a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting the file from local\n",
    "df=prepare.get_fema_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e2904a12",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['nri_ver', 'countyfips', 'countytype', 'oid_', 'statefips', 'nri_id', 'stcofips'] not found in axis\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Input \u001b[0;32mIn [3]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# wrangle df\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m df\u001b[38;5;241m=\u001b[39m\u001b[43mprepare\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprepare_fema\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/codeup-data-science/Census/prepare.py:34\u001b[0m, in \u001b[0;36mprepare_fema\u001b[0;34m(df)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mprepare_fema\u001b[39m(df):\n\u001b[1;32m     33\u001b[0m     \u001b[38;5;66;03m# Drop columns for additional ID columns or version info\u001b[39;00m\n\u001b[0;32m---> 34\u001b[0m     \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m{\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnri_id\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstatefips\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcountytype\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mcountyfips\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     35\u001b[0m \u001b[43m                     \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mnri_ver\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mstcofips\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43moid_\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m     37\u001b[0m     \u001b[38;5;124;03m'''We chose the 3 most costly disasters and removed the others. \u001b[39;00m\n\u001b[1;32m     38\u001b[0m \u001b[38;5;124;03m    We combined lighnting and strong wind to represent a severe storm'''\u001b[39;00m\n\u001b[1;32m     40\u001b[0m     \u001b[38;5;66;03m# Social Vulnerability\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/util/_decorators.py:311\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[1;32m    306\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    307\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39marguments),\n\u001b[1;32m    308\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[1;32m    309\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mstacklevel,\n\u001b[1;32m    310\u001b[0m     )\n\u001b[0;32m--> 311\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/frame.py:4954\u001b[0m, in \u001b[0;36mDataFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4806\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mlabels\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[1;32m   4807\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdrop\u001b[39m(\n\u001b[1;32m   4808\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4815\u001b[0m     errors: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m   4816\u001b[0m ):\n\u001b[1;32m   4817\u001b[0m     \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m   4818\u001b[0m \u001b[38;5;124;03m    Drop specified labels from rows or columns.\u001b[39;00m\n\u001b[1;32m   4819\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4952\u001b[0m \u001b[38;5;124;03m            weight  1.0     0.8\u001b[39;00m\n\u001b[1;32m   4953\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4954\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   4955\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlabels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4956\u001b[0m \u001b[43m        \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4957\u001b[0m \u001b[43m        \u001b[49m\u001b[43mindex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mindex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4958\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4959\u001b[0m \u001b[43m        \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4960\u001b[0m \u001b[43m        \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4961\u001b[0m \u001b[43m        \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   4962\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:4267\u001b[0m, in \u001b[0;36mNDFrame.drop\u001b[0;34m(self, labels, axis, index, columns, level, inplace, errors)\u001b[0m\n\u001b[1;32m   4265\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m axis, labels \u001b[38;5;129;01min\u001b[39;00m axes\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m   4266\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 4267\u001b[0m         obj \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_drop_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlevel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlevel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4269\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   4270\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_update_inplace(obj)\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/generic.py:4311\u001b[0m, in \u001b[0;36mNDFrame._drop_axis\u001b[0;34m(self, labels, axis, level, errors, consolidate, only_slice)\u001b[0m\n\u001b[1;32m   4309\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mdrop(labels, level\u001b[38;5;241m=\u001b[39mlevel, errors\u001b[38;5;241m=\u001b[39merrors)\n\u001b[1;32m   4310\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 4311\u001b[0m         new_axis \u001b[38;5;241m=\u001b[39m \u001b[43maxis\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdrop\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43merrors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43merrors\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4312\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m axis\u001b[38;5;241m.\u001b[39mget_indexer(new_axis)\n\u001b[1;32m   4314\u001b[0m \u001b[38;5;66;03m# Case for non-unique axis\u001b[39;00m\n\u001b[1;32m   4315\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m/opt/homebrew/anaconda3/lib/python3.9/site-packages/pandas/core/indexes/base.py:6644\u001b[0m, in \u001b[0;36mIndex.drop\u001b[0;34m(self, labels, errors)\u001b[0m\n\u001b[1;32m   6642\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39many():\n\u001b[1;32m   6643\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m errors \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m-> 6644\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(labels[mask])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not found in axis\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6645\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m indexer[\u001b[38;5;241m~\u001b[39mmask]\n\u001b[1;32m   6646\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdelete(indexer)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['nri_ver', 'countyfips', 'countytype', 'oid_', 'statefips', 'nri_id', 'stcofips'] not found in axis\""
     ]
    }
   ],
   "source": [
    "# wrangle df\n",
    "df=prepare.prepare_fema(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90538ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution\n",
    "df.hist(figsize=(24, 10), bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "275b23a2",
   "metadata": {},
   "source": [
    "- The majority of people has less than $8,000 per person\n",
    "- The distribution graphs showing a lot outliers in this data. We decided not remove any outliers due to we would like all inclusive data for all states and counties."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f678ba10",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cfb153d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split df into test (20%) and train_validate (80%)\n",
    "tv_df, test = train_test_split(df, test_size=0.2, random_state=123)\n",
    "# split train_validate off into train (70% of 80% = 56%) and validate (30% of 80% = 24%)\n",
    "train, validate = train_test_split(tv_df, test_size=0.3, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71546681",
   "metadata": {},
   "source": [
    "# Wrangle Key Takeaway\n",
    "- The data has 3142 rows, and 26 columns. This project specifically focusing on the Severe Storm, Draught and Hurricane.\n",
    "- The project has included all 50 states' counties and the District of Columbia area. The risk score is retrived it from Federal Emergency Management Agency (FEMA). In our analysis we used three disasters: Drought, Severe Storm, and Hurricane.\n",
    "- Next, we will explore the data and set our target variable==support level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e539c612",
   "metadata": {},
   "source": [
    "# Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38623724",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets make a map before out exploration\n",
    "fig = px.choropleth(df,\n",
    "                    locations='state', \n",
    "                    locationmode=\"USA-states\", \n",
    "                    scope=\"usa\",\n",
    "                    color='support_value',\n",
    "                    color_continuous_scale=\"rdylbu\", \n",
    "                    \n",
    "                    )\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "266a5911",
   "metadata": {},
   "source": [
    "### Takeaway\n",
    "- The science.org research believes that the california will face the mega-flood in near future as climate change worsen\n",
    "    - 'Recent evidence suggests that increases in western United States flood risk caused by anthropogenic warming may have been counteracted in recent decades by natural variability, but that further warming and shifts in natural variability will eventually “unmask” this accumulated increase in regional flood risk (51). Additional work suggests that the response of flood risk to climate change is likely to exhibit threshold behavior, at least in certain climatological and hydrological regimes (52), with a precipitation extremeness threshold dictating whether flood risk decreases (for smaller events, due to the antecedent soil aridification effect of warming temperatures) or increases (for the largest events, due to the overwhelming effect of large increases in precipitation intensity). Both of these considerations are especially germane to California' [science.org](https://www.science.org/doi/10.1126/sciadv.abq0995)\n",
    "- The columbia study and New York Times journalist argued that the New York is not adequately prepared for the next storm like Hurricane Sandy.\n",
    "    - 'The barrier debate comes as New York City is still struggling to respond to Sandy, and the larger need to carefully reshape an entire region’s infrastructure to adapt to climate change. In the more than seven years since the storm killed 72 people and caused 62 billion in damage, agencies have spent just 54 percent of the 14.7 billion allocated by the federal government to help the city recover and prepare for new storms.' [columbia.edu](https://news.climate.columbia.edu/2020/01/21/politics-cost-adapting-climate-change-new-york-city/)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0e0cfb6",
   "metadata": {},
   "source": [
    "### Q1: Whether the dense of the population contributed to the overall funding available or the risk level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618477a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d356dd15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up th eenvironment\n",
    "q1=tv_df.groupby('state',as_index=False).agg(sum)\n",
    "q1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd03172b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bin the risk level to identify whether the funding available also associate with \n",
    "q1['risk_bin'] = pd.qcut(q1.risk_score, 4, labels=['low', 'median', 'hight', 'extremely_high'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec40ea10",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use catplot\n",
    "sns.pointplot(data=q1, x=\"risk_bin\", y=\"pop_density\", dodge=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58c47025",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use catplot\n",
    "sns.pointplot(data=q1, x=\"risk_bin\", y=\"state_funding\", dodge=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e051045",
   "metadata": {},
   "source": [
    "### q1 Key Takeaway\n",
    "- As the risk level rise, the funding will be impacted by the pop_density\n",
    "- The extremely high risk has highest funding when the pop_density is high\n",
    "- It is worth to further explore which region it is belong to and what kind of the disaster contributed to increased funding"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8106cd2",
   "metadata": {},
   "source": [
    "### q2 the cost of the disaster per region per risk level per pop_density"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe7e86d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the environment\n",
    "q2 = tv_df.copy()\n",
    "q2['risk_bin'] = pd.qcut(q2.risk_score, 4, labels=['low', 'median', 'hight', 'extremely_high'])\n",
    "\n",
    "# create the region based on the state's location\n",
    "west = ['WA','OR','CA','ID','NV','MT','WY','UT','AZ','CO','NM']\n",
    "midwest = ['ND','MN','WI','MI','SD','NE','KS','IA','MO','IL','IN','OH']\n",
    "south = ['TX','OK','AR','LA','MS','TN','KY','AL','GA','FL','SC','NC','VA','WV','MD','DE','DC']\n",
    "northeast = ['PA','NJ','NY','CT','MA','RI','VT','NH','ME']\n",
    "west_dict = {i:\"west\" for i in west}\n",
    "midwest_dict = {i:\"midwest\" for i in midwest}\n",
    "south_dict = {i:\"south\" for i in south}\n",
    "northeast_dict = {i:\"northeast\" for i in northeast}\n",
    "d = {**west_dict, **midwest_dict, **south_dict, **northeast_dict}\n",
    "q2['Region'] = q2['state'].map(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23438174",
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.jointplot(data=q2, x=\"pop_density\", y=\"cost\", hue='Region')\n",
    "g.plot_joint(sns.kdeplot, color=\"r\", zorder=0, levels=6)\n",
    "g.plot_marginals(sns.rugplot, color=\"r\", height=-.15, clip_on=False)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4c7dfdd",
   "metadata": {},
   "source": [
    "### q2 Key Takeaway\n",
    "- The South region has most diversed cost versus pop_density\n",
    "- The South region has as the author [Cindy Ermus wrote](https://www.theatlantic.com/technology/archive/2017/08/why-the-gulf-coast-is-uniquely-vulnerable-to-disasters-hurricane-harvey/538374/) in her book,\"The Gulf South, and the Gulf Coast in particular, is bound together by much more than geography or the shared experience of risk and vulnerability to wind, water, erosion, and biological exchanges,” she writes. “More fundamentally, the environment has helped define the region’s identity and largely determined its history, its social fabric, and its economy.”\n",
    "- We can assumed, based on this graph that south region has significant high support value compare to other regions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f6e733",
   "metadata": {},
   "source": [
    "### q3: Based on the q2 takeaway, we are going to make a hypothesis test that whether the south region has higher support value compare to the other regions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a561b91",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set up the environment\n",
    "q3 = tv_df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df00fe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the region based on the state's location\n",
    "west = ['WA','OR','CA','ID','NV','MT','WY','UT','AZ','CO','NM']\n",
    "midwest = ['ND','MN','WI','MI','SD','NE','KS','IA','MO','IL','IN','OH']\n",
    "south = ['TX','OK','AR','LA','MS','TN','KY','AL','GA','FL','SC','NC','VA','WV','MD','DE','DC']\n",
    "northeast = ['PA','NJ','NY','CT','MA','RI','VT','NH','ME']\n",
    "west_dict = {i:\"west\" for i in west}\n",
    "midwest_dict = {i:\"midwest\" for i in midwest}\n",
    "south_dict = {i:\"south\" for i in south}\n",
    "northeast_dict = {i:\"northeast\" for i in northeast}\n",
    "d = {**west_dict, **midwest_dict, **south_dict, **northeast_dict}\n",
    "q3['Region'] = q3['state'].map(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932f3d72",
   "metadata": {},
   "source": [
    "# Hypothesis\n",
    "\n",
    "- H0: South Region's risk score equals or less than non-south region risk score\n",
    "- Ha: South Region's risk score greater than non-south region risk score\n",
    "- Significance Level α is already set to .05\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbf24750",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the environment\n",
    "south_r = q3[q3.Region == 'south'].support_value\n",
    "non_south= q3[q3.Region != 'south'].support_value\n",
    "west=q3[q3.Region == 'west'].support_value\n",
    "midwest=q3[q3.Region == 'midwest'].support_value\n",
    "northeast=q3[q3.Region == 'northeast'].support_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6453a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check variance\n",
    "print(south_r.var())\n",
    "print(non_south.var())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81360b3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put into the test\n",
    "t, p = stats.ttest_ind(south_r, non_south, equal_var=False)\n",
    "t, p\n",
    "p<0.05"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d9c8338",
   "metadata": {},
   "source": [
    "### We reject the null value, that the sourth region's risk score is greater than other regions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18dfa004",
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.kruskal(south, west,northeast,midwest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f6dab08",
   "metadata": {},
   "source": [
    "### Using Kruskal-Wallis test, non-parametric test for ANOVA, also shows us that the mean of Risk Score  from the 4 Regions is significantly different."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fb401ae",
   "metadata": {},
   "source": [
    "### Q3 takeaway\n",
    "- The south region risk score is higher than other regions\n",
    "- The Kruskal-Wallis test also concur that the risk score is significantly different among 4 regions\n",
    "- We can further assumed the supporting level in south region is higher than other region "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa043665",
   "metadata": {},
   "source": [
    "### q4: Based on the q3 takeaway, we are going to make a hypothesis test that whether the south region has higher supporting level compare to the other regions based on their risk_level."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92d2835b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up environment\n",
    "q4 = tv_df.copy()\n",
    "q4['risk_level'] = pd.qcut(q4.risk_score, 4, labels=['low', 'median', 'hight', 'extremely_high'])\n",
    "# create the region based on the state's location\n",
    "west = ['WA','OR','CA','ID','NV','MT','WY','UT','AZ','CO','NM']\n",
    "midwest = ['ND','MN','WI','MI','SD','NE','KS','IA','MO','IL','IN','OH']\n",
    "south = ['TX','OK','AR','LA','MS','TN','KY','AL','GA','FL','SC','NC','VA','WV','MD','DE','DC']\n",
    "northeast = ['PA','NJ','NY','CT','MA','RI','VT','NH','ME']\n",
    "west_dict = {i:\"west\" for i in west}\n",
    "midwest_dict = {i:\"midwest\" for i in midwest}\n",
    "south_dict = {i:\"south\" for i in south}\n",
    "northeast_dict = {i:\"northeast\" for i in northeast}\n",
    "d = {**west_dict, **midwest_dict, **south_dict, **northeast_dict}\n",
    "q4['Region'] = q4['state'].map(d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48721297",
   "metadata": {},
   "source": [
    "H0: support level is not depended on the risk level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f9a6bb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create observed dataframe\n",
    "observed = pd.crosstab(q4.risk_level, q4.Region)\n",
    "observed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1159e01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# chi2 test\n",
    "chi2, p, degf, expected = stats.chi2_contingency(observed)\n",
    "\n",
    "print('Observed\\n')\n",
    "print(observed.values)\n",
    "print('---\\nExpected\\n')\n",
    "print(expected)\n",
    "print('---\\n')\n",
    "print(f'chi^2 = {chi2:.4f}')\n",
    "print(f'p     = {p}')\n",
    "print('P is less than Significance Level:',p<0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b467822",
   "metadata": {},
   "source": [
    "### Q4 key takeawat\n",
    "- The support level is depended on the risk score"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5780485",
   "metadata": {},
   "source": [
    "# Exploration Key takeaway\n",
    "- The south region has higher support level in average compare to the other regions\n",
    "- The risk level is associated with support level"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1eebd369",
   "metadata": {},
   "source": [
    "# Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fa9abcc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data and set up the environment for our scaling\n",
    "X_train, y_train, X_validate, y_validate, X_test, y_test = modeling.train_validate_test(df, 'support_level')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02beb9ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# scale the data for the modeling\n",
    "X_train_scaled, X_validate_scaled, X_test_scaled = modeling.scale_data(X_train, X_validate, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3245a869",
   "metadata": {},
   "outputs": [],
   "source": [
    "# find out the baseline for the model\n",
    "(y_train == 'bottom tier').mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a450065",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setting up my baseline accuracy.  Models must have a better accuracy than this established baseline.\n",
    "print(f'The baseline accuracy for bottom tier counties in all cases within the Climate Risk Assessment dataset is {(y_train == \"bottom tier\").mean():.2}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0880c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use created module to train with different models and find out which model has best accuracy.\n",
    "modeling.modeling(X_train_scaled, y_train, X_validate_scaled, y_validate, 5, 3, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c50d2e6c",
   "metadata": {},
   "source": [
    "### Key takeaway\n",
    "- The Kneighbors Classifier has the highest accuracy when we have it with max_depth=3 and random_state=123\n",
    "- We will going to use the KNN model for our final test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed96374",
   "metadata": {},
   "outputs": [],
   "source": [
    "# putting KNN into final test\n",
    "knn = KNeighborsClassifier(n_neighbors=15)\n",
    "knn.fit(X_train_scaled, y_train)\n",
    "pred_test = knn.predict(X_test_scaled)\n",
    "print('KNN')\n",
    "print('')\n",
    "print('test score: ')\n",
    "print(classification_report(y_test, pred_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51ffb0bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a confusion matrix\n",
    "actual_test=y_test.copy()\n",
    "ConfusionMatrixDisplay(confusion_matrix(actual_test,pred_test),display_labels=knn.classes_).plot()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a1f0c43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict the dataset with our random forest model\n",
    "prob_test = knn.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a3ac6cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a probability dataframe\n",
    "prob_df = pd.DataFrame(prob_test, columns=knn.classes_.tolist())\n",
    "prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4dc30c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset the indext\n",
    "reset_test =test.reset_index()\n",
    "reset_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7771063b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# put two dataframe together\n",
    "test_prob_df = pd.concat([reset_test, prob_df], axis=1)\n",
    "test_prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a426227e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# use the propability result to create a column call predicted\n",
    "test_prob_df['predicted'] = pred_test\n",
    "test_prob_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f725db22",
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrive columns of interest\n",
    "# create the region based on the state's location\n",
    "west = ['WA','OR','CA','ID','NV','MT','WY','UT','AZ','CO','NM']\n",
    "midwest = ['ND','MN','WI','MI','SD','NE','KS','IA','MO','IL','IN','OH']\n",
    "south = ['TX','OK','AR','LA','MS','TN','KY','AL','GA','FL','SC','NC','VA','WV','MD','DE','DC']\n",
    "northeast = ['PA','NJ','NY','CT','MA','RI','VT','NH','ME']\n",
    "west_dict = {i:\"west\" for i in west}\n",
    "midwest_dict = {i:\"midwest\" for i in midwest}\n",
    "south_dict = {i:\"south\" for i in south}\n",
    "northeast_dict = {i:\"northeast\" for i in northeast}\n",
    "d = {**west_dict, **midwest_dict, **south_dict, **northeast_dict}\n",
    "test_prob_df['Region'] = test_prob_df['state'].map(d)\n",
    "csv_df = test_prob_df[['support_level', 'Region','predicted']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57cdfd69",
   "metadata": {},
   "outputs": [],
   "source": [
    "# check the dataframe\n",
    "csv_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f75d6676",
   "metadata": {},
   "source": [
    "### Modeling Takeaway\n",
    "- The KNN is our best model and we use it in out final test\n",
    "- The KNN model achived 90% accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4654f189",
   "metadata": {},
   "source": [
    "# Recommendation, Conclusion, and Next Step\n",
    "\n",
    "## Recommendation\n",
    "- New York and California have the highest support_level. A separate study is recommended for these specific states and their counties within.\n",
    "- Climate change should be our top concern at the all levels of government, and higher amount of funding should be allocated accordingly.\n",
    "- The KNN is our best model in our training, so we used it in our test. The model achived 90% accuracy \n",
    "\n",
    "## Conclusion \n",
    "- The initial data indicated that the New York and California have the highest risk in the United States in terms of funding and studies were found to back up those findings.\n",
    "- The South region has the most diverse cost in damages versus pop_density\n",
    "- The South region has, as the author Cindy Ermus wrote in her book,\"The Gulf South, and the Gulf Coast in particular, is bound together by much more than geography or the shared experience of risk and vulnerability to wind, water, erosion, and biological exchanges,” she writes. “More fundamentally, the environment has helped define the region’s identity and largely determined its history, its social fabric, and its economy.” We can assumed, based on this graph that south region has significant high risk compare to other regions\n",
    "\n",
    "## Next Step\n",
    "- Expand the scope of the data (include more hazards and possibly more geospatial data)\n",
    "- Improve the machine learning model by using other algorithms\n",
    "- Conduct a seperate study regarding New York and California or possibly introduce a model that can look at specific regions at any given time according to the most common hazards found in the area.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4540e01",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
